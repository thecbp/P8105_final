---
title: "Report Rough Draft"
date: "11/19/2018"
output: github_document
---

# Introduction 

Colectomies are surgical procedures that remove all of part of your large intestine. These surgeries are performed for various reasons, ranging from bowel obstruction to colon cancer to just being preventative. Over 250,000 colectomies are performed each year in the United States alone, representing an estimated 10% of the total volume of general surgeries. Given the prolific nature of the surgery, the rate of post-operation complication is astounding: the average rate of complication approached 30% in the last 10 years.[1](https://www.medscape.org/viewarticle/711126)

## Project Motivation

Given this, we wanted to investigate what factors contributed to increasing or decreasing the risk of post-operative complication. We have a dataset on colectomies performed from 2014 - 2016 from multiple hospitals in Michigan. Each row in the dataset represents a single colectomy and a multitude of other information concerning the surgery and the patient. Using this data, we plan to do a regression analysis to figure out which variables have an impact on affecting post-surgery complication.  

# Data Characterization

Before we start our analyses, it's important for us to understand our data in its original form.

## Libraries and settings

```{r lib-imports, message = FALSE }
# Our data analysis toolkit
library(tidyverse)
source("./utils.R")

# Formatting plot output
knitr::opts_chunk$set(
  out.width = "90%"
)

# Set the plot design
theme_set(theme_classic() + 
            theme(legend.position = "bottom", 
                  plot.title = element_text(hjust = 0.5)))
```

Our raw data is contained in `procedure10.csv`, so we'll bring it in and have a look at its size. 

```{r data-import }
colectomies = read.csv(file = './procedure10.csv') 

# TODO: Figure out how to grab data from specific URL (ie OneDrive)
```

In its raw form, `colectomies` has `r nrow(colectomies)` rows and `r ncol(colectomies)` columns. Each row in this dataset corresponds to a single colectomy with an incredible amount of information associated with it. The data has already been purged of identifiable personal information, leaving us with a bevy of laboratory, disease and patient data. 

That being said, many of the columns are useless for regression analysis. Many columns are contain mostly or only missing data, denoted by blank cells or `NA`. Before we can start the variable selection for our model, we need to tidy up the dataset. 

# Data Cleaning

To start the cleaning, we want to remove all columns that contain more than 50% missing values (either blank cells or `NA`). Furthermore, some of the columns actually contain duplicate information from others. These duplicate columns start with `flg` or `e`. 

```{r data-reduction }
is_mostly_intact = function(col) {
  # Account for literal NAs and blanks
  missings = is.na(col) | col == "" | col == "NA"
  return(sum(missings) <= (3084 / 2))
}

tidy_colectomies = colectomies %>% 
  select(-starts_with("flg_"), -starts_with("e_")) %>% 
  select_if(unlist(map(., is_mostly_intact), use.names = FALSE)) %>% 
  prettify_names(.) %>% 
  mutate(any_ssi =  (postop_ssi_super + postop_ssi_deep + 	postop_ssi_organspace) >= 1)
```

After the removal of duplicate columns and columns with more than 50% missing data, `tidy_colectomies` comes to be `r nrow(tidy_colectomies)` rows and `r ncol(tidy_colectomies)` columns. 

Many of the columns have uninuitive or non-descriptive names, so change these up to reflect the information they convey. There are too many columns to rename inline, so we created a function `prettyify_names` in `utils.R` to do it compactly in this report. 



# For checking current dataset in Excel (not include in report)

```{r}
# Sanity check the current
write.csv(tidy_colectomies, file = "./test.csv")
```

# References

